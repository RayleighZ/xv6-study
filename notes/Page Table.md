# Page Table

## Sv39 RISC-V的多级页表

### 分页机制

> 参考：[SV39 多级页表的硬件机制](https://rcore-os.github.io/rCore-Tutorial-Book-v3/chapter4/3sv39-implementation-1.html)

Sv39 RISC-V中，虚拟内存有64位，但是只有低39位可以用来寻址。虚拟内存使用的核心寄存器是`satp`寄存器，长度为8字节，具体结构如下

![](https://rcore-os.github.io/rCore-Tutorial-Book-v3/_images/satp.png)

- `MODE` ：控制 CPU 使用哪种寻址策略，如果此值被设定为0，则代表所有访问均是物理内存地址，当被设定为8时，SV39才会启动分页内存机制。
- `ASID` ：表示地址空间标识符，这里还没有涉及到进程的概念，我们不需要管这个地方；
- `PPN` ：进程进行内存映射需要映射页表，这个页表并不是单纯的一张表格，出于节省内存的考量，这张用作内存映射的表格常被以树的形式存储，这里存储的就是这棵树的root。

虚拟地址和物理地址的具体结构如下

<img src="https://rcore-os.github.io/rCore-Tutorial-Book-v3/_images/sv39-va-pa.png" style="zoom:67%;" /> 

> 为何Sv39分页的RISC-V是39位虚拟地址？
>
> 在64位的RISC-V架构上，虚拟地址确实应该和位宽一致，均为64位，但是对于Sv39分页而言只有低39位是有意义的，SV39 分页模式规定 64 位虚拟地址的 [63:39] 这 25 位必须和第 38 位相同，否则会被认定为是一个不合法的虚拟地址。
>
> 也就是说，所有 264 个虚拟地址中，只有最低的 256GiB （当第 38 位为 0 时）以及最高的 256GiB （当第 38 位为 1 时）是可能通过 MMU 检查的。当我们写软件代码的时候，一个地址的位宽毋庸置疑就是 64 位，我们要清楚可用的只有最高和最低这两部分

虚拟地址的[38:12]位标识用于区分page，而[11:0]位用于标识地址在具体的page中的位置。（个人猜测这样划分page的依据是使得page尽可能的多，并且剩余的位数也刚好可以完成对page的内部寻址，达到没有冗余位的作用），也就是说逻辑上最多可以拥有2^27个分页表，这个Virtual Page也被称为page table entrie，也就是PTEs，每一个PTE为56位，也就是物理内存，其中高44位用于标识物理分页的位置physical page number (PPN)，后续内容为在这个physical page中的具体位置。

也就是说，通过Page Table，将39位虚拟内存映射为了56位物理内存。每4096 byte的内存单元组成

> 为什么是4096 byte？首先，PTE的offset是12位，也就代表着在单个page拥有2^12=4096个寻址，而这些寻址对应的并不是bit，操作的下限是byte，单个page掌控4096 byte的物理内存也就因此而来。
>
>  page table gives the operating system control over virtual-to-physical address translations at the granularity of aligned chunks of 4096 (2^12) bytes. Such a chunk is called a page.

具体的对应关系如下

<img src="https://github.com/RayleighZ/ImageBed/blob/master/virtualAddress.png?raw=true" style="zoom: 33%;" /> 

低39位构成了Sv39的具有实际含义的虚拟内存地址，其中前27位构成了PTE寻址，在总大小为2^27的PTEs中找到对应的某一个PTE，单个PTE指向共4096 byte的内存空间，VA具体对应的PA需要在PTE中通过offset进行标识。

下讨论为什么要使用分级页表（参考文章：[多级页表如何节约内存 ](https://www.polarxiong.com/archives/多级页表如何节约内存.html)）

试考虑一个问题，这样的页表将会占用多大的内存？假设虚拟地址空间为32位，总计有4GB的内存需要映射。然后OS的细粒度划分为4096 byte，也就是4KB，这样的话需要4GB÷4KB=1M的PTE。假设一条PTE长度为4B，则总页表大小为4MB。对于每一个进程而言都需要作此划分，未免开销过大。

如果采用分页内存，从表面上似乎问题并没有得到解决，假设采用二级分页，每一个一级PTE可以控制4096 KB的空间，如果用这些存储二级分页的PTE，则一个一级PTE可以存储1K个二级PTE，故总计需要4GB÷4KB=1M的二级PTE，和1M÷1K=1K的以及PTE，占用的总内存为4MB+4KB，相较前者甚至多出了4KB的内存占用。

多级页表能节约内存并不表现在完备映射之后内存占用少于不分级，而是体现在以下两个方面：1、二级页表可以不存在；2、二级页表可以不在主存

> * 二级页表可以不存在：不同的进程拥有不同的虚拟内存，32位虚拟内存可以提供4GB的运行空间，但并不是所有程序均需如此多的内存，故可以先只映射一部分内存。比如上例中，一级页表可以假装Handle了4GB的内存，但是实际上很多部分的二级页表并没有被实例化，因为高概率程序用不到，可以在程序需要的时候再创建，就可以减少内存占用。为什么不分级就不能这样？个人认为是OS没有操作空间，分集之后通过MMU寻址是去找OS写入的一级页表，寻址的话是可以寻址到的，只不过再从一级页表找到二级页表的时候需要OS解决一下映射问题，但是如果不分级，将直接寻址不到。
> * 二级页表可以不存在于主存：可以将内存页表存储在磁盘中，等到需要的时候再从磁盘中读取。为什么不分级做不到磁盘存储的原因应该是和二级页表可以不存在的原理相同。

多级页表的使用方案如下所示（图示为3级分页内存），L2中的9 bit地址标识再root（stap）所指中的虚拟PTE位置，从L2的寻址中，可以找到第二集页表的位置，L1对应的就是二级页表中的PTE位置，之后以此类推，知道第三级才最终指向PA。

<img src="https://rcore-os.github.io/rCore-Tutorial-Book-v3/_images/sv39-full.png" style="zoom: 50%;" /> 



每条PTE中均含有FLAG位，具体功能在The Book中介绍如下（大概就是：是否真的存在映射，进程是否可读，进程是否可写啥的）

PTE_V indicates whether the PTE is present: if it is not set, a reference to the page causes an exception (i.e. is not allowed). PTE_R controls whether instructions are allowed to read to the page. PTE_W controls whether instructions are allowed to write to the page. PTE_X controls whether the CPU may interpret the content of the page as instructions and execute them. PTE_U controls whether instructions in user mode are allowed to access the page; if PTE_U is not set, the PTE can be used only in supervisor mode.

关于satp寄存器，the book中是如此介绍的（此处的CPU应当做核心理解，如果cpu中的核心公用一个satp，多进程处理真正的并行将不好实现）

To tell the hardware to use a page table, the kernel must write the physical address of the root page-table page into the satp register. Each CPU has its own satp. A CPU will translate all addresses generated by subsequent instructions using the page table pointed to by its own satp.

### MMU与TLB

> 参考文章：[Chapter 3: Page Tables - 知乎   ](https://zhuanlan.zhihu.com/p/351646541)      [TLB与MMU - 菜鸟学院 (noobyard.com)](http://www.noobyard.com/article/p-gstonvwf-rg.html)

MMU（Memory Management Unit，内存管理单元），主要作用是将虚拟内存转换为物理内存，是固定的硬件电路，比如在Sv39中就发挥了上述的三级分页寻址功能。一旦在页表中找到（hit），就通过找到的物理地址寻址到内存中的数据。如果页表中没有找到（miss），表示页表中没有建立这个数据虚拟地址到物理地址的映射，就会触发缺页异常，OS就可以在这个时候建立对应的page

TLB（Translation Lookaside Buffer，俗称快表），其存在目的是加快内存访问（也许有点像CPU的三缓），主要存储的是最近的使用的页表，减少页表查询操作，如果一个需要访问内存中的一个数据，给定这个数据的虚拟地址，查询TLB，发现有（hit），直接得到物理地址，在内存根据物理地址取数据。如果TLB没有这个虚拟地址（miss），那么就只能费力的通过页表来查找了。

如果地址空间发生了切换，TLB就需要被刷新，不然可能会导致寻址错误。

when xv6 changes a page table, it must tell the CPU to invalidate corresponding cached TLB entries. If it didn’t, then at some point later the TLB might use an old cached mapping, pointing to a physical page that in the meantime has been allocated to another process, and as a result, a process might be able to scribble on some other process’s memor

## Kernel address space

接下来将要介绍Kernel是如何形成虚拟虚拟内存空间的。

QEMU中，内存空间的地址大致是[0x80000000,0x86400000]（其中0x86400000是最小的内存终止地址，在xv6中被定义为PHYSTOP），0x80000000之下的地址则用于软硬件接口交互，OS通过读写相关内存来与device交互。

QEMU exposes the device interfaces to software as memory-mapped control registers that sit below 0x80000000 in the physical address space. The kernel can interact with the devices by reading/writing these special physical addresses

kernel部分的虚拟内存映射普遍采用直接映射，也就是使VA等于PA，当然对于部分需求并未采用直接映射，下图为具体的内存映射图

<img src="https://github.com/RayleighZ/ImageBed/blob/master/kernel-space.png?raw=true" style="zoom:50%;" /> 

上图中左侧的RWX分别表示读、写、执行三大权限是否具备，右侧表示当调用到相关内存的时候，具体那些物理内存希望被使用到。在VA和PA中，kernel的最低内存都是0x80000000，这个被称为KERNBASE。直接内存映射简化了kernel访问内存的过程，使得有些时候kernel可以直接将物理内存地址当作虚拟内存使用。Kernel Space中有两部分的内存地址不属于直接映射，他们分别是：

* trampoline：trampoline位于VA的顶端，每一个内存都拥有一个属于自己的trampoline，可能会被随机的映射到RAM的某些位置，trampoline的具体功能将在Chapter 4介绍

* kernel stack pages：每一个进程都有一个属于自己的kernel stack，而这些kernel stack均位于kernel space中，不同的kernel stack之间通过Guard page分割，Guard Pace是不被允许访问的，这样当存在越区访问时会优先崩溃，毕竟数据被错误篡改的威胁要大于kernel异常。其实这里不进行直接映射的原因就在于Guard page，如果直接映射，毫无疑问Guard Page映射过去的PA是无法访问到的，也就是被浪费的。

  however, providing guard pages would involve unmapping virtual addresses that would otherwise refer to physical memory, which would then be hard to use.

## Virtual Address 源码分析

核心源码主要位于vm.c中，核心结构就是pagetable_t，它指向了Page Table的root位置（其实pagetable_t就是一个int数组，存储PTE root数组，大小为512PTEs，从上文中可以了解到，xv6采取三级分页，单级采用9位PPN，也就是说单张页表需要对应2^9=512个PTE，值得一提的是，这里的PTE是uint64，也就是说一个page的大小位8byte×512=4096byte）

walk函数负责通过PTE找到PA，mappages方法则负责加载PTEs，将PTEs加入到映射之中

VA的启动位于main函数中，具体调用过程如下

```c
kinit();         // physical page allocator
kvminit();       // create kernel page table
kvminithart();   // turn on paging
procinit();      // process table
```

VA的起点来自kvminit函数，其代码如下

```c
void
kvminit(void)
{
  //为Kernel形成虚拟内存页表
  kernel_pagetable = kvmmake();
}
```

接下来关注形成页表的`kvmmake`函数，可见kvmmake大致完成了pagetable的形成以及基础数据的映射，包括核心寄存器，kernel相关数据，并最后交由`proc_mapstacks`函数来完成kernel stacks的映射

```c
pagetable_t
kvmmake(void)
{
  pagetable_t kpgtbl;
  //因为现在还没有开启paging，所以这里其实就是物理地址
  kpgtbl = (pagetable_t) kalloc();
  //根据前文的计算，容易得到单张page的大小为4096 byte
  //故这里是在清空整个PAGETABLE
  memset(kpgtbl, 0, PGSIZE);

  // 下面是将一定的资源按照一定的权限写入PAGETABLE中
  // uart registers
  kvmmap(kpgtbl, UART0, UART0, PGSIZE, PTE_R | PTE_W);

  // virtio mmio disk interface
  ......
  
  // map kernel stacks
  proc_mapstacks(kpgtbl);
  
  return kpgtbl;
}
```

`proc_mapstacks`的具体代码如下，这里会将64（系统的最大进程数）个process的stack都分配掉

```c
// Allocate a page for each process's kernel stack.
// Map it high in memory, followed by an invalid
// guard page.
void
proc_mapstacks(pagetable_t kpgtbl) {
  struct proc *p;
  
  for(p = proc; p < &proc[NPROC]; p++) {
    char *pa = kalloc();
    if(pa == 0)
      panic("kalloc");
    uint64 va = KSTACK((int) (p - proc));
    kvmmap(kpgtbl, va, (uint64)pa, PGSIZE, PTE_R | PTE_W);
  }
}
```

之后将调用kvminithart函数来启动paging，具体代码如下

```c
// Switch h/w page table register to the kernel's page table,
// and enable paging.
void
kvminithart()
{
  //正式向satp寄存器中写入pagetable，从这一行开始，就将进入虚拟内存阶段
  w_satp(MAKE_SATP(kernel_pagetable));
  //刷新转译后备缓冲器
  sfence_vma();
}
```

接下来看一下walk函数，其功能是根据64bit VA寻找PTE，代码如下，大致就是MMU的寻址逻辑，但是这里并不是通过异常函数来处理尚未分配的页表，而是直接在此函数中形成新的页表

if the alloc argument is set, walk allocates a new page-table page and puts its physical address in the PTE. It returns the address of the PTE in the lowest layer in the tree

为什么walk函数申请了新的内存之后不需要刷新TLB cache？刷新TLB的契机应该是改变了已有的pagetable，walk中做出的修改属于增加TLB的范畴，就算不刷新也不会造成寻址错误

```c
pte_t *
walk(pagetable_t pagetable, uint64 va, int alloc)
{
  //
  if(va >= MAXVA)
    panic("walk");

  for(int level = 2; level > 0; level--) {
    pte_t *pte = &pagetable[PX(level, va)];
    if(*pte & PTE_V) {
      pagetable = (pagetable_t)PTE2PA(*pte);
    } else {
      //当页表并未被创建时，创建页表
      if(!alloc || (pagetable = (pde_t*)kalloc()) == 0)
        return 0;
      //页表初始化，清零
      memset(pagetable, 0, PGSIZE);
      *pte = PA2PTE(pagetable) | PTE_V;
    }
  }
  return &pagetable[PX(0, va)];
}
```

上面这些代码都必须建立在Kernel Space是直接映射的前提下，比如walk函数中的这一行

```c
pte_t *pte = &pagetable[PX(level, va)];
```

也就是walk函数不断向下一等级寻址的过程，是直接将pa拿出来赋值给va，这样就要求直接映射

之后main函数将调用`procinit()`，其中有一个目的就是将每一个proc的kernel stack映射到虚拟内存加载时分配的kernel space中

> 目前看源码的主要疑惑是：好像很多场景下需要刷新TLB，但是OS没有刷新TLB，貌似用户层的刷新位于trampoline中，后续会看到。and in the trampoline code that switches to a user page table before returning to user space (kernel/trampoline.S:79). 如果每次switch的时候都会刷新一次TLB，应该可以解决我的心头之惑

内核在运行时会分配和释放很多物理内存，xv6将一部分的物理内存，从kernel data结束开始，到PHYSTOP为止，这一部分称为free memory，用于运行时的内存分配。每次分配和回收都**以页为单位**，一页大小4KB，通过一个**空闲物理帧链表free-list**，将空闲的物理帧串起来保存。页表、用户内存、内核栈、管道缓冲区等操作系统组件需要内存时，内核就从free-list上摘下一页或者多页分配给它们；在回收已经分配出去的内存时，这些被回收的物理帧，内核将它们一页页地重新挂到free-list上。

## Process address space

> 参考：[Chapter 3: Page Tables](https://zhuanlan.zhihu.com/p/351646541)

每一个进程都有独立的页表，当xv6切换执行不同的程序时，也会切换satp指向的页表，一个子进程的内存空间地址在0~MAXVA之间，这大概是256GB的内存空间（当然这是虚拟内存空间，实际上这256GB的空间并没有被分配出来，只有当进程尝试使用时才会真正分配，这也是分页内存的优美之处）大部分的PTEs处于PTE_V状态，尚未被加载。

每一个内存拥有单独的VM的好处如下

* 每一个内存的VA将会被映射到不同的PA上，也就是说每个进程的PA是private的，这样就可以实现进程间的内存隔离
* 每一个进程都认为自己将拥有连续的内存空间地址，但是实际上其物理地址很有可能是不连续的
* 通过页表，OS可以将trampoline映射到VA的最顶端，保证所有的进程都可以看得见。

下图是更为详细的process address分布。让我们关注stack部分

<img src="https://github.com/RayleighZ/ImageBed/blob/master/user-process-address-space.png?raw=true" style="zoom: 33%;" /> 

最上面的是：各命令行参数的字符串，指向各命令行参数的指针数组argv[ ]，用于从调用`main(argc, argv[ ])`返回的其它参数（argc、argv指针和伪造的返回pc值）。在初始用户栈的内容被设置好之后，用户程序就返回并开始执行main函数。当然这只是用户进程的stack的一部分，随着main函数的执行以及其他函数的调用，会有更多的变量或者栈帧加入其中。

为了监控用户stack的overflow，xv6在stack下面安放了guard page，如果溢出，则会触发page-fault，现代OS可能会在overflow之后自动为stack补充内存。

## code: sbrk

sbrk是帮助进程减少或者增长内存的系统调用，具体的功能实现在`growproc`函数中，下分析其源码

```c
uint64
sys_sbrk(void)
{
  int addr;
  int n;

  if(argint(0, &n) < 0)
    return -1;
  addr = myproc()->sz;
  if(growproc(n) < 0)
    return -1;
  return addr;
}
```

代行者是growproc，代码如下

```c
// Grow or shrink user memory by n bytes.
// Return 0 on success, -1 on failure.
int
growproc(int n)
{
  uint sz;
  struct proc *p = myproc();

  //这里的sz是process使用的va的大小
  //看上去指向的是空闲的heap
  sz = p->sz;
  //根据n的正负判断是削减内存还是增加内存
  if(n > 0){
    if((sz = uvmalloc(p->pagetable, sz, sz + n)) == 0) {
      return -1;
    }
  } else if(n < 0){
    sz = uvmdealloc(p->pagetable, sz, sz + n);
  }
  p->sz = sz;
  return 0;
}
```

增加内存的代码如下

```c
// Allocate PTEs and physical memory to grow process from oldsz to
// newsz, which need not be page aligned.  Returns new size or 0 on error.
uint64
uvmalloc(pagetable_t pagetable, uint64 oldsz, uint64 newsz)
{
  char *mem;
  uint64 a;

  if(newsz < oldsz)
    return oldsz;
  //PGROUNDUP是计算地址按照pagesize的整数倍向上取整的结果
  oldsz = PGROUNDUP(oldsz);
  for(a = oldsz; a < newsz; a += PGSIZE){
    //分配新的内存
    mem = kalloc();
    if(mem == 0){
      //如果分配失败，就删掉刚刚增加的page并返回
      uvmdealloc(pagetable, a, oldsz);
      return 0;
    }
    //如果分配成功，就手动将page置零，之后再map进去
    memset(mem, 0, PGSIZE);
    if(mappages(pagetable, a, PGSIZE, (uint64)mem, PTE_W|PTE_X|PTE_R|PTE_U) != 0){
      //如果map失败，就删page，free物理内存之后run
      kfree(mem);
      uvmdealloc(pagetable, a, oldsz);
      return 0;
    }
  }
  return newsz;
}
```

删除内存的代码如下

```c
// Deallocate user pages to bring the process size from oldsz to
// newsz.  oldsz and newsz need not be page-aligned, nor does newsz
// need to be less than oldsz.  oldsz can be larger than the actual
// process size.  Returns the new process size.
uint64
uvmdealloc(pagetable_t pagetable, uint64 oldsz, uint64 newsz)
{
  if(newsz >= oldsz)
    return oldsz;

  if(PGROUNDUP(newsz) < PGROUNDUP(oldsz)){
    int npages = (PGROUNDUP(oldsz) - PGROUNDUP(newsz)) / PGSIZE;
    uvmunmap(pagetable, PGROUNDUP(newsz), npages, 1);
  }

  return newsz;
}
```

可以看到真正的执行者是uvmunmap函数，其代码如下

```c
// Remove npages of mappings starting from va. va must be
// page-aligned. The mappings must exist.
// Optionally free the physical memory.
void
uvmunmap(pagetable_t pagetable, uint64 va, uint64 npages, int do_free)
{
  uint64 a;
  pte_t *pte;

  if((va % PGSIZE) != 0)
    panic("uvmunmap: not aligned");

  for(a = va; a < va + npages*PGSIZE; a += PGSIZE){
    //通过walk找到pte
    if((pte = walk(pagetable, a, 0)) == 0)
      panic("uvmunmap: walk");
    if((*pte & PTE_V) == 0)
      panic("uvmunmap: not mapped");
    if(PTE_FLAGS(*pte) == PTE_V)
      panic("uvmunmap: not a leaf");
    if(do_free){
      //free PTE对应的物理内存地址
      uint64 pa = PTE2PA(*pte);
      kfree((void*)pa);
    }
    //将PTE归零
    *pte = 0;
  }
}
```

用户层的page table并不只告诉硬件这里的虚拟内存对应的物理内存地址，同样也记录了那一块物理内存被分配给了这个进程。所以再free内存的时候需要此PTE是否有效/存在。

xv6 uses a process’s page table not just to tell the hardware how to map user virtual addresses, but also as the only record of which physical memory pages are allocated to that process. That is the reason why freeing user memory (in uvmunmap) requires examination of the user page table.

## code: exec

exec是一个system call，负责将指定路径下的指定格式文件（xv6中为ELF文件）加载进内存中，下面简单分析一下源码调用过程

首先exec将会校验ELF文件格式是否正确，代码如下

```c
// Check ELF header
if(readi(ip, 0, (uint64)&elf, 0, sizeof(elf)) != sizeof(elf))
goto bad;
if(elf.magic != ELF_MAGIC)
goto bad;
```

exec会尝试根据elf文件格式读取elf，之后读取ELF head，并校验其是否等于ELF MAGIC，其中，ELF MAGIC是类似于JAVA类加载机制中校验部分的JAVA魔数（0xCAFEBABE），就向JAVA魔数用于鉴定某一个文件是否是class文件一样，xv6在这里通过ELF 魔数（宏定义为0x464C457FU，也就是0x7FELF的小端改写）来验证某一个文件是否是ELF文件，如果文件头是ELF MAGIC，xv6则认定其为ELF文件。

> 继续上次的ELF文件解析
>
> 参考文章：[ELF文件解析（一）：Segment和Section - JollyWing](https://www.cnblogs.com/jiqingwu/p/elf_format_research_01.html)
>
> 下图描述了ELF文件的大体格式
>
> ![](https://img2018.cnblogs.com/blog/417313/201810/417313-20181012154909093-954664315.png) 
>
> 左侧为链接视图，可以理解为ELF文件的目标代码文件布局，右边是执行视图，可以理解为可执行文件的内容视图，值得注意的是代码

在通过了ELF校验之后，xv6将试图为这个ELF文件分配内存，根据第一章的知识易知为ELF文件中的Segment段，这里使用的是loadseg方法，相关调用如下

```c
if((sz1 = uvmalloc(pagetable, sz, ph.vaddr + ph.memsz)) == 0)
    goto bad;
sz = sz1;
if((ph.vaddr % PGSIZE) != 0)
    goto bad;
if(loadseg(pagetable, ph.vaddr, ip, ph.off, ph.filesz) < 0)
    goto bad;
```

首先通过uvmalloc为process address space扩容，之后校验是否内存对齐，最后通过loadseg将ELF文件中的内容加载进内存中

